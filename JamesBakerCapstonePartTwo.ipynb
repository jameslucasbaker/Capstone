{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Two - Analysing related article sentiment and bias\n",
    "This workbook considers the second part of the project - taking a group of related articles and assessing their sentiment and scope for bias.\n",
    "In an ideal world, where we each want to have a fair and balanced view on every topic, we would hope to read articles covering a range of perspectives on the same story.\n",
    "Having developed the means to find related articles in part one of this project, this part applies several sentiment analysis techniques to look at the distribution of their sentiment. It ultimately arrives at a score to convey how balanced the coverage of a story is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "### Imports\n",
    "First some of the required packages must be imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import nltk as nl\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import statistics\n",
    "import random\n",
    "\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter configuration\n",
    "The parameters used to control the NLP-related calculations, and to specify the domain for any grid search are captured in the runParams dict. This includes specification of the location of the key input files.\n",
    "The runParams dict is converted into an sklearn ParameterGrid, even if there is no grid search requirement (in which case it's processed as a single scenario grid search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runParams={'sentiment_library':   ['google','vader','stanford'],\n",
    "           'input_file':          ['./data/articles.csv'],\n",
    "           'article_id_list':     [[120639,80103,25225,21502,57362,120636]],\n",
    "           'sentiment_sentences': [5],\n",
    "           'article_stats':       [False]}\n",
    "\n",
    "# Use parameter grid even if there is only set of parameters\n",
    "parameterGrid=ParameterGrid(runParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File loader for news corpus\n",
    "\n",
    "This is the same function as used in Part One."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInputDataAndDisplayStats(filename,processDate,printSummary=False):\n",
    "\n",
    "\tdf=pd.read_csv(filename)\n",
    "\n",
    "\tdf=df.drop_duplicates('content')\n",
    "\tdf=df[~df['content'].isnull()]\n",
    "\n",
    "\t# There are a large number of junk articles, many of which either don't make sense or\n",
    "\t# just contain a headline - as such they are useless for this analysis and may distort\n",
    "\t# results if left in place\n",
    "\tdf=df[df['content'].str.len()>=200]\n",
    "\n",
    "\t# Find and remove summary NYT \"briefing\" articles to avoid confusing the clustering\n",
    "\ttargetString=\"(Want to get this briefing by email?\"\n",
    "\tdf['NYT summary']=df['content'].map(lambda d: d[:len(targetString)]==targetString)\n",
    "\tdf=df[df['NYT summary']==False]\n",
    "\n",
    "\t# The following removes a warning that appears in many of the Atlantic articles.\n",
    "\t# Since it is commonly at the beginning, it brings a lot of noise to the search for similar articles\n",
    "\t# And subsequently to the assessment of sentiment\n",
    "\ttargetString=\"For us to continue writing great stories, we need to display ads.             Please select the extension that is blocking ads.     Please follow the steps below\"\n",
    "\tdf['content']=df['content'].str.replace(targetString,'')\n",
    "\n",
    "\t# This is also for some Atlantic articles for the same reasons as above\n",
    "\ttargetString=\"This article is part of a feature we also send out via email as The Atlantic Daily, a newsletter with stories, ideas, and images from The Atlantic, written specially for subscribers. To sign up, please enter your email address in the field provided here.\"\n",
    "\tdf=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "\t# This is also for some Atlantic articles for the same reasons as above\n",
    "\ttargetString=\"This article is part of a feature we also send out via email as Politics  Policy Daily, a daily roundup of events and ideas in American politics written specially for newsletter subscribers. To sign up, please enter your email address in the field provided here.\"\n",
    "\tdf=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "\t# More Atlantic-specific removals (for daily summaries with multiple stories contained)\n",
    "\tdf=df[df['content'].str.contains(\"To sign up, please enter your email address in the field\")==False]\n",
    "\n",
    "\t# Remove daily CNN summary\n",
    "\ttargetString=\"CNN Student News\"\n",
    "\tdf=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "\tif printSummary:\n",
    "\t\tprint(\"\\nArticle counts by publisher:\")\n",
    "\t\tprint(df['publication'].value_counts())\n",
    "\n",
    "\t\tprint(\"\\nArticle counts by date:\")\n",
    "\t\tprint(df['date'].value_counts())\n",
    "\t\t\n",
    "\t# Restrict to articles on the provided input date.\n",
    "\t# This date is considered mandatory for topic clustering but is not required for sentiment\n",
    "\t# since sentiment only processes a specified list of articles.\n",
    "\t# For topic clustering it is essential to have the date as it is\n",
    "\t# enormously significant in article matching.\n",
    "\tif processDate!=None:\n",
    "\t\tdf=df[df['date']==processDate]\n",
    "\tdf.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\t# Remove non-ASCII characters\n",
    "\tdf['content no nonascii']=df['content'].map(lambda x: removeNonASCIICharacters(x))\n",
    "\n",
    "\tprint(\"\\nFinal dataset:\\n\\nDate:\",processDate,\"\\n\")\n",
    "\tprint(df['publication'].value_counts())\n",
    "\n",
    "\treturn df\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def removeNonASCIICharacters(textString): \n",
    "    return \"\".join(i for i in textString if ord(i)<128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the articles from the corpus\n",
    "In addition the function will return the number of articles per publication (for the requested run date). Here we see there is a relatively good mix of political viewpoints covered. More discussion of this is provided in the project report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articleDataFrame=getInputDataAndDisplayStats(runParams['input_file'][0],None,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the sentiment analysis classes\n",
    "Three NLP libraries are supported for this part of the project: Vader, Google, and Stanford Core NLP. Classes will be defined to enable each of them.\n",
    "### Parent Sentiment Analysis Class\n",
    "The main sentiment analyser class provides a consistent wrapper and interface around classes specific to the various NLP libraries. Its constructor creates and embeds an instance of the appropriate class. It provides additional standard interfaces to trigger the analysis on an article, and to return the overall score for that article.\n",
    "Additionally it manages the scaling of results across classes in order to facilitate consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyser():\n",
    "\n",
    "\tscaleMin=-1.\n",
    "\tscaleMax=1.\n",
    "\n",
    "    # Initializer / Instance attributes\n",
    "\tdef __init__(self, library):\n",
    "\t\tif library=='google':\n",
    "\t\t\tself.analyser=GoogleSentimentAnalyser()\n",
    "\t\telif library=='stanford':\n",
    "\t\t\tself.analyser=StanfordSentimentAnalyser()\n",
    "\t\telif library=='vader':\n",
    "\t\t\tself.analyser=NLTKVaderSentimentAnalyser()\n",
    "\t\telse:\n",
    "\t\t\tprint(\"ERROR - NO RECOGNISED LIBRARY\")\n",
    "\n",
    "\tdef getOverallArticleScore(self,articleResults):\n",
    "\n",
    "\t\t# Google returns a document score, but it is an int, which is useful when comparing documents\n",
    "\t\t# Hence computing the average of the sentences here instead\n",
    "\t\t# Google's document score is here: articleResults.document_sentiment.score\n",
    "\t\tnumSentences=0.\n",
    "\t\ttotalSentScore=0.\n",
    "\t\tfor sentence in articleResults:\n",
    "\t\t\tnumSentences+=1\n",
    "\t\t\ttotalSentScore+=self.analyser.getSentenceScoreFromResults(sentence)\n",
    "\n",
    "\t\tvalue=(totalSentScore/numSentences-self.analyser.scaleMin)/(self.analyser.scaleMax-self.analyser.scaleMin)\n",
    "\t\treturn self.scaleMin+value*(self.scaleMax-self.scaleMin)\n",
    "\n",
    "\tdef generateResults(self,textToAnalyse):\n",
    "\t\treturn self.analyser.generateResults(textToAnalyse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google NLP library class\n",
    "The following class pertains to the use of the Google Cloud Library. The class provides interfaces to the relevant Google methods and packages results for return to the parent. Note that this needs to have appropriate Google Cloud Platform credentials available - see the project report for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleSentimentAnalyser():\n",
    "\n",
    "\tscaleMin=-1.\n",
    "\tscaleMax=1.\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.client=language.LanguageServiceClient()\n",
    "\t\treturn\n",
    "\n",
    "\tdef generateResults(self,textToAnalyse):\n",
    "\t\tdocument=types.Document(\n",
    "\t\t\t\t\t\t\t\tcontent=textToAnalyse,\n",
    "\t\t\t\t\t\t\t\ttype=enums.Document.Type.PLAIN_TEXT\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\t\treturn self.client.analyze_sentiment(document=document).sentences\n",
    "\n",
    "\tdef getSentenceScoreFromResults(self,sentenceResults):\n",
    "\t\treturn sentenceResults.sentiment.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vader library class\n",
    "This class does the same thing for using the VADER sentiment analyser packaged with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLTKVaderSentimentAnalyser():\n",
    "# Per NLTK Vader user guide: https://pypi.org/project/vaderSentiment/\n",
    "# Typical threshold values (used in the literature cited on this page) are: \n",
    "#. **positive sentiment**: ``compound`` score >= 0.05 \n",
    "#. **neutral sentiment**: (``compound`` score > -0.05) and (``compound`` score < 0.05) \n",
    "#. **negative sentiment**: ``compound`` score <= -0.05 \n",
    "\n",
    "\tscaleMin=-1.\n",
    "\tscaleMax=1.\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.nltkVaderAnalyser=SentimentIntensityAnalyzer()\n",
    "\t\treturn\n",
    "\n",
    "\tdef generateResults(self,textToAnalyse):\n",
    "\t\tss=[]\n",
    "\t\tfor sentence in nl.sent_tokenize(textToAnalyse):\n",
    "\t\t\tss.append(self.nltkVaderAnalyser.polarity_scores(sentence))\n",
    "\t\treturn ss\n",
    "\n",
    "\tdef getSentenceScoreFromResults(self,sentenceResults):\n",
    "\t\treturn sentenceResults['compound']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford Core NLP library class\n",
    "And similarly for using Stanford Core NLP. Note that this needs to have the Stanford Core NLP server running locally in order for it to work. See the instructions in the project report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanfordSentimentAnalyser():\n",
    "\n",
    "\tscaleMin=0.\n",
    "\tscaleMax=4.\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tfrom pycorenlp import StanfordCoreNLP\n",
    "\t\tself.nlp=StanfordCoreNLP('http://localhost:9000')\n",
    "\t\treturn\n",
    "\n",
    "\tdef generateResults(self,textToAnalyse):\n",
    "\t\treturn self.nlp.annotate(textToAnalyse,\n",
    "\t\t\t\t\t\t\t\tproperties={\n",
    "\t\t\t\t\t\t\t\t\t\t\t 'annotators': 'sentiment',\n",
    "\t\t\t\t\t\t\t\t\t\t\t 'outputFormat': 'json',\n",
    "\t\t\t\t\t\t\t\t\t\t\t 'timeout': 100000,  # NB The original example had 1000 and that caused time-out errors\n",
    "\t\t\t\t\t\t\t\t\t\t\t})[\"sentences\"]\n",
    "\n",
    "\tdef getSentenceScoreFromResults(self,sentenceResults):\n",
    "\t\treturn int(sentenceResults[\"sentimentValue\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance/Score computation methods\n",
    "These are to determine a score for how well a set of documents manages to cover a variety of perspectives..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePopulationBalanceScore(articleScoreDict,sentimentClass):\n",
    "\t# Extract values from dict, then normalise to be within -1 to +1\n",
    "\t# Then compute population standard deviation as the balance score\n",
    "\tpopulation=[-1.+(x-sentimentClass.scaleMin)/(sentimentClass.scaleMax-sentimentClass.scaleMin)*(1.-(-1.)) for x in articleScoreDict.values()]\n",
    "\treturn statistics.pstdev(population)\n",
    "\n",
    "def computePopulationBalanceScoreHistoMean(articleScoreDict,sentimentClass):\n",
    "\t# Extract values from dict, then normalise to be within -1 to +1\n",
    "\t# Then compute population standard deviation as part of the balance score\n",
    "\tnumBuckets=len(articleScoreDict)\n",
    "\tarticleValues=pd.Series(articleScoreDict)\n",
    "\t\n",
    "\t# Based on 10,000 random article samples, Google's sentiment score for these articles lies within +/- 0.86\n",
    "\t# So, scale all scores by dividing by that value to rescale to +/- 1.00 before computing balance score\n",
    "\t# Ideally this should factored in at the individual NLP library class level \n",
    "\tarticleValues=articleValues/0.86\n",
    "\n",
    "\tpopulatedBuckets=0\n",
    "\tfor i in range(numBuckets):\n",
    "\t\tbucketFrom=sentimentClass.scaleMin+i*(sentimentClass.scaleMax-sentimentClass.scaleMin)/numBuckets\n",
    "\t\tbucketTo=bucketFrom+(sentimentClass.scaleMax-sentimentClass.scaleMin)/numBuckets\n",
    "\t\t# The following is to ensure the top of the highest bucket is counted somewhere\n",
    "\t\t# and doesn't fall out due to treatment of inequalities in ranges\n",
    "\t\tif bucketTo==sentimentClass.scaleMax:\n",
    "\t\t\tbucketTo+=0.001\n",
    "\t\tnumSamples=((bucketFrom<=articleValues) & (articleValues<bucketTo)).sum()\n",
    "\t\tif numSamples>0:\n",
    "\t\t\tpopulatedBuckets+=1\n",
    "\n",
    "\t# Score computed as proportion of buckets which are populated (more buckets implies a more balanced view)\n",
    "\t# This has a value between 0 and 1.\n",
    "\t# This is in turn multiplied by the distance between the mean and 1.\n",
    "\t# So, if mean is in center (i.e. at 0) then things are balanced, so score is not decreased\n",
    "\t# Otherwise, score is decreased proportionately\n",
    "\treturn (populatedBuckets/numBuckets * (1.-abs(articleValues.mean())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Story map loading\n",
    "The story map file is an optional way to provide the algorithm with:\n",
    "- A list of stories\n",
    "- Each story has a name (for reference/convenience)\n",
    "- Each story contains a list of articles that pertain to that story\n",
    "\n",
    "The objective is ultimately to process each story, and within each story to measure the sentiment of each article, then (still within the story) to compute the score for the balance of the coverage (of that story).\n",
    "\n",
    "This section of code is taken from the other workbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupStoryMapAndReportList(args=None,reportArticleList=None,storyMapFileName=None):\n",
    "\t# Story Map is used in fitting if grid search is applied (As ground truth)\n",
    "\t# It is also used in graph if no threshold provided (to determine colours, not to determine location)\n",
    "\t# Report Article List is used at the end to create a report with, for each\n",
    "\t# article in the list, the set of articles within tolerance, and the key words for each\n",
    "\tif args==None:\n",
    "\t\tarticleList=reportArticleList\n",
    "\t\tfileName=storyMapFileName\n",
    "\telse:\n",
    "\t\tarticleList=args['article_id_list']\n",
    "\t\tfileName=args['story_map_validation']\n",
    "\n",
    "\treportArticleList=articleList\n",
    "\tif fileName!=None:\n",
    "\t\tstoryMap=readStoryMapFromFile(fileName)\n",
    "\t\tif reportArticleList==None:\n",
    "\t\t\treportArticleList=[]\n",
    "\t\t\tfor story, articleList in storyMap.items():\n",
    "\t\t\t\treportArticleList.append(articleList[0])\n",
    "\telse:\n",
    "\t\tstoryMap=None\n",
    "\treturn storyMap,reportArticleList\n",
    "\n",
    "def readStoryMapFromFile(filename):\n",
    "\treturn readDictFromCsvFile(filename,'StoryMap')\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def readGridParameterRangeFromFile(filename):\n",
    "\treturn readDictFromCsvFile(filename,'GridParameters')\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def readDictFromCsvFile(filename,schema):\n",
    "\tgridParamDict={}\n",
    "\twith open(filename, 'r') as f:\n",
    "\t\tfor row in f:\n",
    "\t\t\trow=row[:-1] # Exclude the carriage return\n",
    "\t\t\trow=row.split(\",\")\n",
    "\t\t\tkey=row[0]\n",
    "\t\t\tvals=row[1:]\n",
    "\t\t\t\n",
    "\t\t\tif schema=='GridParameters':\n",
    "\t\t\t\tif key in ['story_threshold','tfidf_maxdf']:\n",
    "\t\t\t\t\tfinalVals=list(float(n) for n in vals)\n",
    "\t\t\t\telif key in ['ngram_max','tfidf_mindf','max_length']:\n",
    "\t\t\t\t\tfinalVals=list(int(n) for n in vals)\n",
    "\t\t\t\telif key in ['lemma_conversion','tfidf_binary']:\n",
    "\t\t\t\t\tfinalVals=list(str2bool(n) for n in vals)\n",
    "\t\t\t\telif key in ['parts_of_speech']:\n",
    "\t\t\t\t\tlistlist=[]\n",
    "\t\t\t\t\tfor v in vals:\n",
    "\t\t\t\t\t\tlistlist.append(v.split(\"+\"))\n",
    "\t\t\t\t\tfinalVals=listlist\n",
    "\t\t\t\telif key in ['tfidf_norm','nlp_library']:\n",
    "\t\t\t\t\tfinalVals=vals\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(key)\n",
    "\t\t\t\t\tprint(\"KEY ERROR\")\n",
    "\t\t\t\t\treturn\n",
    "\t\t\telif schema=='StoryMap':\n",
    "\t\t\t\tfinalVals=list(int(n) for n in vals)\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(schema)\n",
    "\t\t\t\tprint(\"SCHEMA ERROR\")\n",
    "\t\t\t\treturn\n",
    "\t\t\t\n",
    "\t\t\tgridParamDict[key]=finalVals\n",
    "\treturn gridParamDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the story map from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storyMap,reportArticleList=setupStoryMapAndReportList(storyMapFileName='storyMapForValidation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the story map we see that it forms a dict containing a key corresponding to the name of the story and a value containing a list of the article IDs germane to that story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for story, articleList in storyMap.items():\n",
    "    print(story,\":\",articleList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment story map with user requested specific article list\n",
    "A requested article list is explicitly part of the input parameters and may vary through each iteration of the main loop. This data may not be consistent with any provided story map file. So the following function will be required in order to reconcile any differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapseRequestedArticleListIntoStoryList(requestedArticleList,storyMap):\n",
    "\t# Check that the explicitly requested articles are all contained in the storyList\n",
    "\t# If they aren't, add a new story to contain them\n",
    "\n",
    "\t# If the storyMap was empty, it will be None,\n",
    "\t# so initialise as a dictionary ready for adding new values\n",
    "\tif storyMap==None:\n",
    "\t\tnewStoryMap={}\n",
    "\telse:\n",
    "\t\tnewStoryMap=storyMap.copy()\n",
    "\n",
    "\tfound=False\n",
    "\tfor story,articleListFromMap in newStoryMap.items():\n",
    "\t\tif len(articleListFromMap)==len(requestedArticleList):\n",
    "\t\t\ty=sum([x in articleListFromMap for x in requestedArticleList])\n",
    "\t\t\tif y==len(articleListFromMap):\n",
    "\t\t\t\tfound=True\n",
    "                \n",
    "\t# If there is no complete story exactly matching then add a new story to the list\n",
    "\t# With the first article ID as the key (arbitrarily)\n",
    "\tif not found:\n",
    "\t\tnewStoryMap[requestedArticleList[0]]=requestedArticleList\n",
    "\treturn newStoryMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN PROCESS\n",
    "Note that the earlier steps need to have been run before this step in order to populate the following:\n",
    "- articleDataFrame\n",
    "- parameterGrid\n",
    "\n",
    "Also, recall that for Google Cloud Platform to work the necessary credentials need to have been created and referenced in the environment.\n",
    "\n",
    "And for Stanford Core NLP to work, the Stanford Java server needs to be running locally.\n",
    "\n",
    "- java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -timeout 1000000\n",
    "\n",
    "Vader will operate without any additional constraints, providing the Python environment has all the correct packages, per my project requirements file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,currentParams in enumerate(parameterGrid):\n",
    "\tif len(parameterGrid)>1:\n",
    "\t\tprint(\"Combination:\",i+1,\"of\",len(parameterGrid))\n",
    "\t\tprint(currentParams)\n",
    "\n",
    "\t# The base storyMap may be appended with an iteration specific\n",
    "\t# article list, depending on the request details in the parameter grid\n",
    "\titerationStoryMap=collapseRequestedArticleListIntoStoryList(currentParams['article_id_list'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstoryMap)\n",
    "\n",
    "\tsentimentAnalyser=SentimentAnalyser(currentParams['sentiment_library'])\n",
    "\n",
    "\tfor story,articleList in iterationStoryMap.items():\n",
    "\t\tarticleSentScores={}\n",
    "\t\tprint(\"ANALYSING STORY:\",story,\"using\",currentParams['sentiment_library'])\n",
    "\t\tprint(\"Number of articles in story:\",len(articleList))\n",
    "\t\tfor article in articleList:\n",
    "\n",
    "\t\t\tarticleContent=articleDataFrame[articleDataFrame['id']==article].iloc[0]['content']\n",
    "\n",
    "\t\t\t# if requested, only use the first few sentences for analysis\n",
    "\t\t\tif currentParams['sentiment_sentences']!=None:\n",
    "\t\t\t\tarticleSentences=nl.sent_tokenize(articleContent)\n",
    "\t\t\t\ttextToAnalyse=' '.join(articleSentences[:currentParams['sentiment_sentences']])\t\n",
    "\t\t\telse:\n",
    "\t\t\t\ttextToAnalyse=articleContent\n",
    "\n",
    "\t\t\tresults=sentimentAnalyser.generateResults(textToAnalyse)\n",
    "\n",
    "\t\t\tarticleSentScores[article]=sentimentAnalyser.getOverallArticleScore(results)\n",
    "\n",
    "\t\t# Sort and display results\n",
    "\t\tsortedArticleSentScores=sorted(articleSentScores.items(), key=operator.itemgetter(1))\n",
    "\t\tprint(\"\\nArticle sentiments, most positive first:\")\n",
    "\t\tfor article in reversed(sortedArticleSentScores):\n",
    "\t\t\tprint(article[0],\":\", round(article[1],3),articleDataFrame[articleDataFrame['id']==article[0]].iloc[0]['publication'])\n",
    "\n",
    "\t\t# This only works because each article's score is constrained to be in -1 to +1\n",
    "\t\t# So maximum possible population standard deviation is 1 and minimum is 0\n",
    "\t\t# Should arguably build this into the classes somewhere, but so far I don't have any\n",
    "\t\t# classes that pertain to the population, rather than to an individual article\n",
    "\t\t# I could do the calculation here based on the class min and max... (and building a function to consume the list) \n",
    "\t\tprint(\"\\nBALANCE SCORE:\",round(computePopulationBalanceScoreHistoMean(articleSentScores,SentimentAnalyser),3)*100.,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article inspection\n",
    "To inspect the raw content of one of the articles, substitute the desired ID in the following command and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "articleDataFrame[articleDataFrame['id']==80101]['content'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Capstone)",
   "language": "python",
   "name": "capstonepython3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
